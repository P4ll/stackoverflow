{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/p4l/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import logging\n",
    "import pickle\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora, models\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from scipy.stats import entropy\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "from scipy.special import (entr, rel_entr)\n",
    "from numpy import (arange, putmask, ravel, ones, shape, ndarray, zeros, floor,\n",
    "                   logical_and, log, sqrt, place, argmax, vectorize, asarray,\n",
    "                   nan, inf, isinf, NINF, empty)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "np.random.seed(2020)\n",
    "\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "DOC_COUNT = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = STOPWORDS.union(set(['use', 'be', 'work', 'user', 'try', 'cell',\n",
    "                                     'row', 'want', 'item', 'go', 'get', 'add', 'went', 'tried',\n",
    "                                    'return', 'sort', 'test', 'run', 'check', 'click', 'hour', 'minute', 'second',\n",
    "                                    'version', 'app', 'paragraph', 'error', 'log', 'press',\n",
    "                                    'need', 'feed', 'thank', 'way', 'like', 'kill', 'help']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/p4l/work/stackoverflow/\"\n",
    "base_model = base_path + \"models_data/\"\n",
    "base_dataset = base_path + \"dataset/\"\n",
    "base_model_lda = base_model + \"lda/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_entropy(pk, qk=None, base=None, axis=0):\n",
    "    pk = asarray(pk)\n",
    "    pk = 1.0*pk / np.sum(pk, axis=axis, keepdims=True)\n",
    "    if qk is None:\n",
    "        vec = entr(pk)\n",
    "    else:\n",
    "        qk = asarray(qk)\n",
    "        #if qk.shape != pk.shape:\n",
    "            #raise ValueError(\"qk and pk must have same shape.\")\n",
    "        qk = 1.0*qk / np.sum(qk, axis=axis, keepdims=True)\n",
    "        vec = rel_entr(pk, qk)\n",
    "    S = np.sum(vec, axis=axis)\n",
    "    if base is not None:\n",
    "        S /= log(base)\n",
    "    return S\n",
    "\n",
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    # lets keep with the p,q notation above\n",
    "    p = query[None,:].T # take transpose\n",
    "    q = matrix.T # transpose matrix\n",
    "\n",
    "    #new = np.zeros((q.shape[0], q.shape[1]))\n",
    "    #new[:q.shape[0], :1] = p\n",
    "    #p = new\n",
    "    \n",
    "    m = 0.5*(p + q)\n",
    "    return np.sqrt(0.5*(m_entropy(p,m) + m_entropy(q,m)))\n",
    "\n",
    "def get_most_similar_documents(query,matrix,k=10):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
    "    out = [dist for dist in sims if not math.isnan(dist)]\n",
    "    return sims.argsort()[:k], sorted(sims, reverse=True) # the top k positional index of the smallest Jensen Shannon distances\n",
    "\n",
    "def clear_text(text):\n",
    "    text = re.sub('<code>(.|\\n)*?<\\/code>', '', text)\n",
    "    text = re.sub(r'(\\<(/?[^>]+)>)', '', text)\n",
    "    text = re.sub(\"[\\'\\\"\\\\/\\@\\%\\(\\)\\~\\`\\{\\}]\", '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = clear_text(text)\n",
    "    result = []\n",
    "    #result = [token in gensim.utils.simple_preprocess(text, deacc=True) if ((token not in gensim.parsing.preprocessing.STOPWORDS) and len(token) > 1) == True]\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if (token not in my_stop_words) and len(token) > 1:\n",
    "            #result.append(lemmatize_stemming(token))\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tags(text):\n",
    "    if not isinstance(text, str) and math.isnan(text):\n",
    "        return ''\n",
    "    if text == '' or text == ' ':\n",
    "        return text\n",
    "    else:\n",
    "        return text.replace('|', ' ')\n",
    "\n",
    "def add_string(text, tags, n=3):\n",
    "    tags = split_tags(tags)\n",
    "    tags = ' ' + tags\n",
    "    i = 0\n",
    "    for i in range(n):\n",
    "        if i % 2 == 0:\n",
    "            text += tags\n",
    "        else:\n",
    "            text = tags + text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(base_dataset + 'large_data.csv')\n",
    "if DOC_COUNT != -1:\n",
    "    data = data.head(DOC_COUNT)\n",
    "\n",
    "titles = data['post_title'].values\n",
    "tags = data['post_tags'].map(split_tags).values\n",
    "data = [add_string(row['post_body'], row['post_tags'], 3) for _, row in data.iterrows()]\n",
    "#data = data['post_body'].values\n",
    "\n",
    "[data.append(z) for z in titles]\n",
    "[data.append(z) for z in tags]\n",
    "[data.append(z) for z in tags]\n",
    "[data.append(z) for z in tags]\n",
    "[data.append(z) for z in tags]\n",
    "\n",
    "#data = np.array(data)\n",
    "#data = np.append(data, titles)\n",
    "#data = np.append(data, tags)\n",
    "#data = np.append(data, tags)\n",
    "#data = np.append(data, tags)\n",
    "#data = np.append(data, tags)\n",
    "\n",
    "print(len(data))\n",
    "del titles\n",
    "del tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['', 'database', 'rdbms', 'ordbms<p>After', 'mounting', 'the', 'database', 'I', 'tried', 'executing', 'these', 'command-', 'alter', 'database', 'open;', 'emanating', 'from', 'a', 'shutdown', 'initialization', 'error', 'but', 'my', 'oracle', 'kept', 'returning', 'an', 'ORA-00600:', 'internal', 'error', 'code,', 'arguments', '[dbkif_find_next_record_1],', '[],', '[],', '[].\\nPlease,', 'how', 'do', 'I', 'proceed', 'from', 'here?</p>', 'database', 'rdbms', 'ordbms', 'database', 'rdbms', 'ordbms']\n",
      "\n",
      "\n",
      "tokenized and lemmatized document: \n",
      "['database', 'rdbms', 'ordbmsafter', 'mounting', 'database', 'executing', 'command', 'alter', 'database', 'open', 'emanating', 'shutdown', 'initialization', 'oracle', 'kept', 'returning', 'ora', 'internal', 'code', 'arguments', 'proceed', 'database', 'rdbms', 'ordbms', 'database', 'rdbms', 'ordbms']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = data[3]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\ntokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_docs = np.array([preprocess(x) for x in data])\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(processed_docs, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags and not token.lemma_ in my_stop_words])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = make_bigrams(processed_docs)\n",
    "processed_docs = make_trigrams(processed_docs)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "processed_docs = lemmatization(processed_docs, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=300,\n",
    "                                           random_state=100,\n",
    "                                           update_every=2,\n",
    "                                           chunksize=50000,\n",
    "                                           passes=15)\n",
    "\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(datapath(base_model_lda + \"model_semi_final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_bow(text):\n",
    "    text = preprocess(text)\n",
    "    text = make_trigrams([text])[0]\n",
    "    text = lemmatization([text])[0]\n",
    "    bow_vector = dictionary.doc2bow(text)\n",
    "    return bow_vector\n",
    "\n",
    "def test_texts(text1, text2):\n",
    "    bow1 = get_text_bow(text1)\n",
    "    bow2 = get_text_bow(text2)\n",
    "    for index, score in sorted(lda_model[bow1], key=lambda tup: -1*tup[1]):\n",
    "        print(f\"index: {index}, score {score}\")\n",
    "        #print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "    print(\"_________________________________________\")\n",
    "    for index, score in sorted(lda_model[bow2], key=lambda tup: -1*tup[1]):\n",
    "        print(f\"index: {index}, score {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(base_dataset + 'large_data.csv')\n",
    "data = data.head(DOC_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_n = 765\n",
    "test_texts(data.loc[row_n, 'post_title'], data.loc[row_n, 'post_body'])\n",
    "print(\"______________________________\")\n",
    "print(data.loc[row_n, 'post_title'])\n",
    "print(\"______________________________\")\n",
    "print(data.loc[row_n, 'post_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
